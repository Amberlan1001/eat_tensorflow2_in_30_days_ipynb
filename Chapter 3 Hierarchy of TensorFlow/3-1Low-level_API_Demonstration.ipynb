{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1 Low-level API: Demonstration\n",
    "\n",
    "The examples below use low-level APIs in TensorFlow to implement a linear regression model.\n",
    "\n",
    "Low-level API includes tensor operator, graph and automatic differentiates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Print time stamps\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "n = 400\n",
    "\n",
    "# Generate testing dataset\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @ represents matrix multiplication; add Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================16:27:30\n",
      "epoch = 1000  loss = 2.60077739\n",
      "w = [[1.96938241]\n",
      " [-0.983276486]]\n",
      "b = 1.86474025\n",
      "\n",
      "================================================================================16:27:31\n",
      "epoch = 2000  loss = 2.08383226\n",
      "w = [[1.9742254]\n",
      " [-0.983455658]]\n",
      "b = 2.5554719\n",
      "\n",
      "================================================================================16:27:33\n",
      "epoch = 3000  loss = 2.01372981\n",
      "w = [[1.97600842]\n",
      " [-0.983521402]]\n",
      "b = 2.809834\n",
      "\n",
      "================================================================================16:27:34\n",
      "epoch = 4000  loss = 2.00422359\n",
      "w = [[1.9766649]\n",
      " [-0.983544946]]\n",
      "b = 2.90350342\n",
      "\n",
      "================================================================================16:27:35\n",
      "epoch = 5000  loss = 2.00293422\n",
      "w = [[1.97690666]\n",
      " [-0.983553588]]\n",
      "b = 2.9379971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Debug in dynamic graph\n",
    "\n",
    "w = tf.Variable(tf.random.normal(w0.shape))\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Loss in forward propagation\n",
    "            Y_hat = X@w + b\n",
    "            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   \n",
    "\n",
    "        # Gradient by back propagation\n",
    "        dloss_dw,dloss_db = tape.gradient(loss,[w,b])\n",
    "        # Update parameters using gradient descent method\n",
    "        w.assign(w - 0.001*dloss_dw)\n",
    "        b.assign(b - 0.001*dloss_db)\n",
    "        if epoch%1000 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\" loss =\",loss,)\n",
    "            tf.print(\"w =\",w)\n",
    "            tf.print(\"b =\",b)\n",
    "            tf.print(\"\")\n",
    "            \n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================16:27:38\n",
      "epoch = 1000  loss = 2.60776782\n",
      "w = [[1.9693377]\n",
      " [-0.983274758]]\n",
      "b = 1.85836804\n",
      "\n",
      "================================================================================16:27:38\n",
      "epoch = 2000  loss = 2.08478\n",
      "w = [[1.97420871]\n",
      " [-0.983455]]\n",
      "b = 2.55312562\n",
      "\n",
      "================================================================================16:27:38\n",
      "epoch = 3000  loss = 2.01385856\n",
      "w = [[1.97600222]\n",
      " [-0.983521163]]\n",
      "b = 2.80897\n",
      "\n",
      "================================================================================16:27:38\n",
      "epoch = 4000  loss = 2.00424099\n",
      "w = [[1.97666264]\n",
      " [-0.983544886]]\n",
      "b = 2.90318608\n",
      "\n",
      "================================================================================16:27:38\n",
      "epoch = 5000  loss = 2.0029366\n",
      "w = [[1.97690582]\n",
      " [-0.983553588]]\n",
      "b = 2.93788052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Accelerate the execution by converting the dynamic graph to static using Autograph\n",
    "\n",
    "w = tf.Variable(tf.random.normal(w0.shape))\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "@tf.function\n",
    "def train(epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Loss in forward propagation\n",
    "            Y_hat = X@w + b\n",
    "            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   \n",
    "\n",
    "        # Gradient by back propagation\n",
    "        dloss_dw,dloss_db = tape.gradient(loss,[w,b])\n",
    "        # Update parameters using gradient descent method\n",
    "        w.assign(w - 0.001*dloss_dw)\n",
    "        b.assign(b - 0.001*dloss_db)\n",
    "        if epoch%1000 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch =\",epoch,\" loss =\",loss,)\n",
    "            tf.print(\"w =\",w)\n",
    "            tf.print(\"b =\",b)\n",
    "            tf.print(\"\")\n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
