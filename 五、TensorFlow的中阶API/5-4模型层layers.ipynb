{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-4,模型层layers\n",
    "\n",
    "深度学习模型一般由各种模型层组合而成。\n",
    "\n",
    "tf.keras.layers内置了非常丰富的各种功能的模型层。例如，\n",
    "\n",
    "layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout\n",
    "\n",
    "layers.Conv2D,layers.MaxPooling2D,layers.Conv1D\n",
    "\n",
    "layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectional等等。\n",
    "\n",
    "如果这些内置模型层不能够满足需求，我们也可以通过编写tf.keras.Lambda匿名模型层或继承tf.keras.layers.Layer基类构建自定义的模型层。\n",
    "\n",
    "其中tf.keras.Lambda匿名模型层只适用于构造没有学习参数的模型层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、内置模型层\n",
    "\n",
    "一些常用的内置模型层简单介绍如下。\n",
    "\n",
    "**基础层**\n",
    "\n",
    "* Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)\n",
    "\n",
    "* Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。\n",
    "\n",
    "* Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。\n",
    "\n",
    "* BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。\n",
    "\n",
    "* SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。\n",
    "\n",
    "* Input：输入层。通常使用Functional API方式构建模型时作为第一层。\n",
    "\n",
    "* DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。\n",
    "\n",
    "* Flatten：压平层，用于将多维张量压成一维。\n",
    "\n",
    "* Reshape：形状重塑层，改变输入张量的形状。\n",
    "\n",
    "* Concatenate：拼接层，将多个张量在某个维度上拼接。\n",
    "\n",
    "* Add：加法层。\n",
    "\n",
    "* Subtract： 减法层。\n",
    "\n",
    "* Maximum：取最大值层。\n",
    "\n",
    "* Minimum：取最小值层。\n",
    "\n",
    "\n",
    "**卷积网络相关层**\n",
    "\n",
    "* Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数\n",
    "\n",
    "* Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数\n",
    "\n",
    "* Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数\n",
    "\n",
    "* SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。\n",
    "\n",
    "* DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。\n",
    "\n",
    "* Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。\n",
    "\n",
    "* LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。\n",
    "\n",
    "* MaxPool2D: 二维最大池化层。也称作下采样层。池化层无可训练参数，主要作用是降维。\n",
    "\n",
    "* AveragePooling2D: 二维平均池化层。\n",
    "\n",
    "* GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。\n",
    "\n",
    "* GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。\n",
    "\n",
    "\n",
    "**循环网络相关层**\n",
    "\n",
    "* Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。\n",
    "\n",
    "* LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。\n",
    "\n",
    "* GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。\n",
    "\n",
    "* SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。\n",
    "\n",
    "* ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。\n",
    "\n",
    "* Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。\n",
    "\n",
    "* RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。\n",
    "\n",
    "* LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。\n",
    "\n",
    "* GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。\n",
    "\n",
    "* Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、自定义模型层\n",
    "\n",
    "\n",
    "如果自定义模型层没有需要被训练的参数，一般推荐使用Lamda层实现。\n",
    "\n",
    "如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现。\n",
    "\n",
    "Lambda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单。\n",
    "\n",
    "Lambda层的正向逻辑可以使用Python的lambda函数来表达，也可以用def关键字定义函数来表达。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:53:03.612258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-12-17 16:53:03.688848: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd9b10b4760 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-12-17 16:53:03.688867: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,regularizers\n",
    "\n",
    "mypower = layers.Lambda(lambda x:tf.math.pow(x,2))\n",
    "mypower(tf.range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer的子类化一般需要重新实现初始化方法，Build方法和Call方法。下面是一个简化的线性层的范例，类似Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    #build方法一般定义Layer需要被训练的参数。    \n",
    "    def build(self, input_shape): \n",
    "        self.w = self.add_weight(\"w\",shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True) #注意必须要有参数名称\"w\",否则会报错\n",
    "        self.b = self.add_weight(\"b\",shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(Linear,self).build(input_shape) # 相当于设置self.built = True\n",
    "\n",
    "    #call方法一般定义正向传播运算逻辑，__call__方法调用了它。  \n",
    "    @tf.function\n",
    "    def call(self, inputs): \n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    #如果要让自定义的Layer通过Functional API 组合成模型时可以被保存成h5模型，需要自定义get_config方法。\n",
    "    def get_config(self):  \n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(None, 8)\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.compute_output_shape(input_shape = (None,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "{'name': 'linear_2', 'trainable': True, 'dtype': 'float32', 'units': 16}\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units = 16)\n",
    "print(linear.built)\n",
    "#如果built = False，调用__call__时会先调用build方法, 再调用call方法。\n",
    "linear(tf.random.uniform((100,64))) \n",
    "print(linear.built)\n",
    "config = linear.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.input_shape:  (None, 2)\n",
      "model.output_shape:  (None, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "linear (Linear)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "#注意该处的input_shape会被模型加工，无需使用None代表样本数量维\n",
    "model.add(Linear(units = 1,input_shape = (2,)))  \n",
    "print(\"model.input_shape: \",model.input_shape)\n",
    "print(\"model.output_shape: \",model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16842256]\n",
      " [0.48468253]]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"sgd\",loss = \"mse\",metrics=[\"mae\"])\n",
    "print(model.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16842256]\n",
      " [0.48468253]]\n"
     ]
    }
   ],
   "source": [
    "# 保存成 h5模型\n",
    "model.save(\"./data/linear_model.h5\",save_format = \"h5\")\n",
    "model_loaded_keras = tf.keras.models.load_model(\n",
    "    \"./data/linear_model.h5\",custom_objects={\"Linear\":Linear})\n",
    "print(model_loaded_keras.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/linear_model/assets\n",
      "[[0.16842256]\n",
      " [0.48468253]]\n"
     ]
    }
   ],
   "source": [
    "# 保存成 tf模型\n",
    "model.save(\"./data/linear_model\",save_format = \"tf\")\n",
    "model_loaded_tf = tf.keras.models.load_model(\"./data/linear_model\")\n",
    "print(model_loaded_tf.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
